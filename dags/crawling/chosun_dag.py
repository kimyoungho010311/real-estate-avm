from airflow import DAG
from airflow.decorators import task
from airflow.sdk import Variable
#from airflow.providers.postgres.hooks.postgres import PostgresHook
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
#from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    NoSuchElementException,
    TimeoutException,
    StaleElementReferenceException,
    WebDriverException,
    #ElementClickInterceptedException,
)
from datetime import datetime, timedelta 
import time
import re
#from io import StringIO
import pandas as pd
from tasks.db import save_to_db

MAX_PAGE = int(Variable.get("MAX_PAGE"))
DRIVER_PATH = Variable.get("DRIVER_PATH")
# ì˜¤ëŠ˜ë°œí–‰ëœ ì‹ ë¬¸ê¸°ì‚¬ì™€ ë¹„êµë¥¼ í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
yesterday_str = (datetime.today() - timedelta(days=1)).strftime("%Y/%m/%d")

dag_owner = 'Ian_Kim'

default_args = {'owner': dag_owner,
        'depends_on_past': False,
        'retries': 3,
        'retry_delay': timedelta(minutes=5)
        }

with DAG(dag_id='chosun',
        default_args=default_args,
        description='ì¡°ì„ ì¼ë³´ í¬ë¡¤ë§',
        start_date=datetime(2022,2,2),
        schedule='30 7 * * *',
        catchup=False,
        tags=['crawling']
):

    @task
    def chosun():

        options = Options() 
        # GUI ì—†ì´ ì‹¤í–‰ - ë°±ì—”ë“œ/ì„œë²„ ìë™í™”
        options.add_argument("--headless")
        # GPU ê°€ì† ê¸°ëŠ¥ off - ì•ˆì •ì„± ê°œì„ 
        options.add_argument("--disable-gpu")
        # Chromeì„ sandbox ì—†ì´ ì‹¤í–‰í•¨ - ê¶Œí•œ ì˜¤ë¥˜ íšŒí”¼(ì£¼ì˜)
        options.add_argument("--no-sandbox")

        try:
            service = Service(executable_path=DRIVER_PATH)
            driver = webdriver.Chrome(service=service, options=options)
            wait = WebDriverWait(driver, 5)
        except WebDriverException as e:
            print(f"[FATAL] Failed to initialize WebDriver: {e}")
            return

        try:
            driver.get("https://www.chosun.com/economy/real_estate/?page=1")
        except Exception as e:
            print(f"[FATAL] Failed to load initial page: {e}")
            driver.quit()
            return

        PAGES_PER_SCREEN = 10
        current = 1
        hrefs = []

        # URL ìˆ˜ì§‘
        while current <= MAX_PAGE:
            for i in range(current, min(current + PAGES_PER_SCREEN, MAX_PAGE + 1)):
                try:
                    page_button = driver.find_element(By.ID, str(i))
                    page_button.click()
                    print(f"[+] Clicked page {i}")
                    time.sleep(2)
                except NoSuchElementException: # ì§€ì •í•œ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ ë°œìƒ
                    print(f" Page button {i} not found.")
                    continue
                except Exception as e:
                    print(f" Unexpected error clicking page {i}: {e}")
                    continue

                try:
                    links = driver.find_elements(By.CSS_SELECTOR, "a")
                    for link in links:
                        try:
                            href = link.get_attribute("href")
                            if href:
                                hrefs.append(href)
                        except StaleElementReferenceException: # ì´ë¯¸ ì°¾ì€ ì›¹ ìš”ì†Œê°€ DOMì—ì„œ ì‚¬ë¼ì ¸ ë” ì´ìƒ ìœ ìš”í•˜ì§€ ëª»í•œ ê²½ìš°
                            continue  # ìš”ì†Œê°€ ì‚¬ë¼ì§„ ê²½ìš° ë¬´ì‹œ
                except Exception as e: # íŠ¹ì • ì˜ˆì™¸ ì™¸ì—ë„ ì˜ˆìƒì¹˜ ëª»í•œ ëª¨ë“  ì˜ˆì™¸ë¥¼ í¬ê´„ì ìœ¼ë¡œ ì²˜ë¦¬í•¨.
                    print(f" Failed to extract links on page {i}: {e}")
                    continue

            # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ í´ë¦­
            if current + PAGES_PER_SCREEN <= MAX_PAGE:
                try:
                    next_page_btn = driver.find_element(
                        By.XPATH,
                        '//*[@id="main"]/div[2]/section/div/div/div/div[21]/div/div[3]/button'
                    )
                    next_page_btn.click()
                    print("[â†’] Clicked next page button")
                    time.sleep(2)
                except NoSuchElementException: # ì§€ì •í•œ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ ë°œìƒ
                    print(" Next page button not found.")
                except Exception as e: # ì§€ì •ëœ ì˜ˆì™¸ ë§ê³ ë„ ëª¨ë“  ì˜ˆì™¸ ì§€ì •
                    print(f" Failed to click next page button: {e}")

            current += PAGES_PER_SCREEN

        # ì¤‘ë³µ ì œê±°
        #https://www.chosun.com/economy/real_estate/2025/08/22/7XO543AOTM4IRLMPABDCAMDUQA/:
        article_links = list(set([href for href in hrefs if "/economy/real_estate/20" in href]))
        filtered_links = []
        

        for link in article_links:
            # ë§í¬ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
            match = re.search(r'/(\d{4}/\d{2}/\d{2})/', link)
            if match:
                link_date = match.group(1)
                if link_date == yesterday_str:
                    filtered_links.append(link)

        # filtered_linksì—ëŠ” ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì‚¬ë§Œ ë‚¨ìŒ
        article_links = filtered_links

        print(f"ğŸ”— Total collected article URLs: {len(article_links)}")

        article = {}

        for i, url in enumerate(article_links):
            try:
                driver.get(url)
                time.sleep(2)

                # ë³¸ë¬¸ ì¶”ì¶œ
                section = driver.find_element(By.CSS_SELECTOR, 'section.article-body')
                paragraphs = section.find_elements(By.TAG_NAME, 'p')
                full_text = "\n".join(p.text.strip() for p in paragraphs if p.text.strip())

                # URLì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                parts = url.split('/')
                year, month, day = parts[5], parts[6], parts[7]  # /economy/real_estate/YYYY/MM/DD/
                date = f"{year}-{month}-{day}"

                # ë”•ì…”ë„ˆë¦¬ ì €ì¥
                article[url] = {
                    'date': date,
                    'content': full_text,
                    'publisher': 'ì¡°ì„ ì¼ë³´'
                }   
                print(f"[{i+1}/{len(article_links)}] Crawled: {url}")
                
            except NoSuchElementException:
                print(f" Article structure not found in {url}")
            except TimeoutException: # í˜ì´ì§€ ë¡œë”© ì†ë„ê°€ ë„ˆë¬´ ëŠë¦´ ë•Œ ë°œìƒ
                print(f" Timeout when loading {url}")
            except Exception as e:
                print(f" Failed to crawl content from {url}: {e}")
                continue

        driver.quit()
        df = pd.DataFrame([
            {"url": url, "date": data["date"], "content": data["content"], "publisher": "ì¤‘ì•™ì¼ë³´"}
            for url, data in article.items()
        ])

        return df
    

    chosun_task = chosun()
    save_to_db_task = save_to_db(chosun_task)

    chosun_task >> save_to_db_task