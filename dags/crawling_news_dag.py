from airflow import DAG
from airflow.decorators import task
from airflow.operators.empty import EmptyOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from datetime import datetime, timedelta 
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    NoSuchElementException,
    StaleElementReferenceException,
    TimeoutException,
    WebDriverException,
    ElementClickInterceptedException,
)
from io import StringIO
import pandas as pd
import csv, time, os , re

dag_owner = 'Ian Kim'

default_args = {'owner': dag_owner,
        'depends_on_past': False,
        #'retries': 2,
        #'retry_delay': timedelta(seconds=3)
        }

DRIVER_PATH = '/usr/bin/chromedriver'
MAX_PAGE = 1
BUCKET_NAME = "ian-geonewsapt"

with DAG(dag_id='crawling_news',
        default_args=default_args,
        description='ë‰´ìŠ¤ í¬ë¡¤ë§',
        start_date=datetime(2019,1,1),
        #schedule='* * * * *',
        catchup=False,
        tags=['.']
) as dag:

    @task
    def chosun():

        options = Options() 
        # GUI ì—†ì´ ì‹¤í–‰ - ë°±ì—”ë“œ/ì„œë²„ ìë™í™”
        options.add_argument("--headless")
        # GPU ê°€ì† ê¸°ëŠ¥ off - ì•ˆì •ì„± ê°œì„ 
        options.add_argument("--disable-gpu")
        # Chromeì„ sandbox ì—†ì´ ì‹¤í–‰í•¨ - ê¶Œí•œ ì˜¤ë¥˜ íšŒí”¼(ì£¼ì˜)
        options.add_argument("--no-sandbox")

        try:
            service = Service(executable_path=DRIVER_PATH)
            driver = webdriver.Chrome(service=service, options=options)
            wait = WebDriverWait(driver, 5)
        except WebDriverException as e:
            print(f"[FATAL] Failed to initialize WebDriver: {e}")
            return

        try:
            driver.get("https://www.chosun.com/economy/real_estate/?page=1")
        except Exception as e:
            print(f"[FATAL] Failed to load initial page: {e}")
            driver.quit()
            return

        PAGES_PER_SCREEN = 10
        current = 1
        hrefs = []

        # URL ìˆ˜ì§‘
        while current <= MAX_PAGE:
            for i in range(current, min(current + PAGES_PER_SCREEN, MAX_PAGE + 1)):
                try:
                    page_button = driver.find_element(By.ID, str(i))
                    page_button.click()
                    print(f"[+] Clicked page {i}")
                    time.sleep(2)
                except NoSuchElementException: # ì§€ì •í•œ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ ë°œìƒ
                    print(f" Page button {i} not found.")
                    continue
                except Exception as e:
                    print(f" Unexpected error clicking page {i}: {e}")
                    continue

                try:
                    links = driver.find_elements(By.CSS_SELECTOR, "a")
                    for link in links:
                        try:
                            href = link.get_attribute("href")
                            if href:
                                hrefs.append(href)
                        except StaleElementReferenceException: # ì´ë¯¸ ì°¾ì€ ì›¹ ìš”ì†Œê°€ DOMì—ì„œ ì‚¬ë¼ì ¸ ë” ì´ìƒ ìœ ìš”í•˜ì§€ ëª»í•œ ê²½ìš°
                            continue  # ìš”ì†Œê°€ ì‚¬ë¼ì§„ ê²½ìš° ë¬´ì‹œ
                except Exception as e: # íŠ¹ì • ì˜ˆì™¸ ì™¸ì—ë„ ì˜ˆìƒì¹˜ ëª»í•œ ëª¨ë“  ì˜ˆì™¸ë¥¼ í¬ê´„ì ìœ¼ë¡œ ì²˜ë¦¬í•¨.
                    print(f" Failed to extract links on page {i}: {e}")
                    continue

            # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ í´ë¦­
            if current + PAGES_PER_SCREEN <= MAX_PAGE:
                try:
                    next_page_btn = driver.find_element(
                        By.XPATH,
                        '//*[@id="main"]/div[2]/section/div/div/div/div[21]/div/div[3]/button'
                    )
                    next_page_btn.click()
                    print("[â†’] Clicked next page button")
                    time.sleep(2)
                except NoSuchElementException: # ì§€ì •í•œ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ ë°œìƒ
                    print(" Next page button not found.")
                except Exception as e: # ì§€ì •ëœ ì˜ˆì™¸ ë§ê³ ë„ ëª¨ë“  ì˜ˆì™¸ ì§€ì •
                    print(f" Failed to click next page button: {e}")

            current += PAGES_PER_SCREEN

        # ì¤‘ë³µ ì œê±°
        article_links = list(set([href for href in hrefs if "/economy/real_estate/20" in href]))
        print(f"ğŸ”— Total collected article URLs: {len(article_links)}")

        article = {}

        for i, url in enumerate(article_links):
            try:
                driver.get(url)
                time.sleep(2)

                # ë³¸ë¬¸ ì¶”ì¶œ
                section = driver.find_element(By.CSS_SELECTOR, 'section.article-body')
                paragraphs = section.find_elements(By.TAG_NAME, 'p')
                full_text = "\n".join(p.text.strip() for p in paragraphs if p.text.strip())

                # URLì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                parts = url.split('/')
                year, month, day = parts[5], parts[6], parts[7]  # /economy/real_estate/YYYY/MM/DD/
                date = f"{year}-{month}-{day}"

                # ë”•ì…”ë„ˆë¦¬ ì €ì¥
                article[url] = {
                    'date': date,
                    'content': full_text,
                    'publisher': 'ì¡°ì„ ì¼ë³´'
                }   
                print(f"[{i+1}/{len(article_links)}] Crawled: {url}")
                
            except NoSuchElementException:
                print(f" Article structure not found in {url}")
            except TimeoutException: # í˜ì´ì§€ ë¡œë”© ì†ë„ê°€ ë„ˆë¬´ ëŠë¦´ ë•Œ ë°œìƒ
                print(f" Timeout when loading {url}")
            except Exception as e:
                print(f" Failed to crawl content from {url}: {e}")
                continue

        driver.quit()
        df = pd.DataFrame([
            {"url": url, "date": data["date"], "content": data["content"], "publisher": "ì¤‘ì•™ì¼ë³´"}
            for url, data in article.items()
        ])

        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False)

        s3_hook = S3Hook(aws_conn_id='s3_conn')
        key = f"news_dataframe/chosun_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        s3_hook.load_string(csv_buffer.getvalue(), key=key, bucket_name=BUCKET_NAME, replace=True)
        return ''
    
    @task
    def dong_a():
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")

        try:
            service = Service(executable_path=DRIVER_PATH)
            driver = webdriver.Chrome(service=service, options=options)
            wait = WebDriverWait(driver, 5)
        except WebDriverException as e:
            print(f"[FATAL] Failed to initialize WebDriver: {e}")
            return

        article_links = set()
        cutoff_date = 20200901  # ê¸°ì¤€ ë‚ ì§œ

        # í˜ì´ì§€ ìˆœíšŒ
        for page in range(1, MAX_PAGE + 1):
            offset = (page - 1) * 20 + 1
            url = f"https://www.donga.com/news/Economy/RE?p={offset}&prod=news&ymd=&m="
            try:
                driver.get(url)
                print(f" Visiting page {page} -> {url}")
                time.sleep(2)

                links = driver.find_elements(By.CSS_SELECTOR, "a")
                for link in links:
                    href = link.get_attribute("href")
                    if href and "https://www.donga.com/news/Economy/article/all/" in href:
                        match = re.search(r'/all/(\d{8})/', href)
                        if match:
                            article_date = int(match.group(1))
                            if article_date >= cutoff_date:
                                article_links.add(href)

            except Exception as e:
                print(f" Failed to process page {page}: {e}")

        article_links = list(article_links)
        print(f"ğŸ”— Total collected article URLs: {len(article_links)}")

        # ë³¸ë¬¸ ìˆ˜ì§‘
        article = {}
        for i, url in enumerate(article_links):
            try:
                driver.get(url)
                time.sleep(2)
                section = driver.find_element(By.CSS_SELECTOR, 'section.news_view')
                driver.execute_script("""
                    const section = arguments[0];
                    const tags = section.querySelectorAll('script, style, iframe, div.a1, div.view_ad06, div.view_m_adA, div.view_m_adB');
                    tags.forEach(tag => tag.remove());
                """, section)
                full_text = section.get_attribute('innerText').strip()
                if not full_text:
                    full_text = "ë³¸ë¬¸ ì—†ìŒ"
                    print(f" ({i+1}/{len(article_links)}) Crawled: {url} | ë³¸ë¬¸ ì—†ìŒ")
                else:
                    print(f" ({i+1}/{len(article_links)}) Crawled: {url} | {len(full_text)}ì ì¶”ì¶œ")
            except Exception as e:
                full_text = "ì ‘ê·¼ ì‹¤íŒ¨"
                print(f" ({i+1}/{len(article_links)}) URL ì ‘ê·¼ ì‹¤íŒ¨: {url} | ì—ëŸ¬: {e}")

            # URLì—ì„œ ë‚ ì§œ ì¶”ì¶œ
            match = re.search(r'/all/(\d{8})/', url)
            date = f"{match.group(1)[:4]}-{match.group(1)[4:6]}-{match.group(1)[6:]}" if match else "Unknown"

            article[url] = {
                'date': date,
                'content': full_text,
                'publisher' : 'ë™ì•„ì¼ë³´'
            }

        driver.quit()
        # DataFrame ë³€í™˜
        df = pd.DataFrame([
            {"url": url, "date": data["date"], "content": data["content"], "publisher": "ì¤‘ì•™ì¼ë³´"}
            for url, data in article.items()
        ])

        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False)

        s3_hook = S3Hook(aws_conn_id='s3_conn')
        key = f"news_dataframe/dong_a_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        s3_hook.load_string(csv_buffer.getvalue(), key=key, bucket_name=BUCKET_NAME, replace=True)
        return ''
    
    @task
    def joonang():

        options = Options()
        options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")

        try:
            service = Service(executable_path=DRIVER_PATH)
            driver = webdriver.Chrome(service=service, options=options)
            wait = WebDriverWait(driver, 5)
        except WebDriverException as e:
            print(f"[FATAL] Failed to initialize WebDriver: {e}")
            return None

        driver.get("https://www.joongang.co.kr/realestate?page=1")
        try:
            wait.until(EC.presence_of_element_located((
                By.CSS_SELECTOR,
                '#container > section > div.contents_bottom.float_left > section:nth-child(2) > nav > ul > li.page_next > a'
            )))
        except TimeoutException:
            print(" Initial page load timeout")
            driver.quit()
            return None

        article_links = []

        # ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
        for page in range(MAX_PAGE):
            try:
                print(f" ({page+1}/{MAX_PAGE}) í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
                time.sleep(2)
                a_tags = driver.find_elements(By.CSS_SELECTOR, '#story_list a')
                for a in a_tags:
                    href = a.get_attribute('href')
                    if href:
                        article_links.append(href)

                next_page_btn = driver.find_element(
                    By.CSS_SELECTOR,
                    '#container > section > div.contents_bottom.float_left > section:nth-child(2) > nav > ul > li.page_next > a'
                )
                next_page_btn.click()
            except (ElementClickInterceptedException, NoSuchElementException) as e:
                print(f" ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ: {e}")
                break
            except Exception as e:
                print(f" í˜ì´ì§€ {page+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        article_links = list(set(article_links))
        print(f" ì´ {len(article_links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")

        # ê¸°ì‚¬ ë‚´ìš© ìˆ˜ì§‘
        article = {}
        for i, url in enumerate(article_links):
            try:
                driver.get(url)
                time.sleep(2)
                article_section = driver.find_element(By.CSS_SELECTOR, "#article_body")
                paragraphs = article_section.find_elements(By.TAG_NAME, 'p')
                full_text = "\n".join(p.text.strip() for p in paragraphs if p.text.strip())

                time_element = driver.find_element(By.CSS_SELECTOR, 'time[itemprop="datePublished"]')
                published_date = time_element.get_attribute('datetime')

                article[url] = {
                    "content": full_text,
                    "date": published_date,
                    "publisher" : 'ì¤‘ì•™ì¼ë³´'
                }
                print(f" ({i+1}/{len(article_links)}) Crawled: {url} | {len(full_text)}ì ì¶”ì¶œ")


            except Exception as e:
                print(f" ({i+1}/{len(article_links)}) URL ì ‘ê·¼ ì‹¤íŒ¨: {url} | ì—ëŸ¬: {e}")
                article[url] = {
                    "content": "ì ‘ê·¼ ì‹¤íŒ¨",
                    "date": None
                }

        driver.quit()
        print(" ì¤‘ì•™ì¼ë³´ í¬ë¡¤ë§ ì™„ë£Œ")

        df = pd.DataFrame([
            {"url": url, "date": data["date"], "content": data["content"], "publisher": "ì¤‘ì•™ì¼ë³´"}
            for url, data in article.items()
        ])

        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False)

        s3_hook = S3Hook(aws_conn_id='s3_conn')
        key = f"news_dataframe/joonang_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        s3_hook.load_string(csv_buffer.getvalue(), key=key, bucket_name=BUCKET_NAME, replace=True)
        return ''

    @task
    def korea_eco():
    # Chrome headless ì„¤ì •
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")

        service = Service(executable_path=DRIVER_PATH)
        driver = webdriver.Chrome(service=service, options=options)
        wait = WebDriverWait(driver, 5)

        article_links = []

        def collect_links_from_category(url, category_name):
            driver.get(url)
            for i in range(MAX_PAGE):
                try:
                    print(f"[{category_name}] {i+1}ë²ˆì§¸ í˜ì´ì§€ ë§í¬ ìˆ˜ì§‘ì¤‘...")
                    # í˜ì´ì§€ ì„ íƒ
                    wait.until(EC.presence_of_element_located(
                        (By.CSS_SELECTOR, '#contents > div.select-paging > div.page-select.txt-num > div > select')
                    ))
                    select_element = driver.find_element(
                        By.CSS_SELECTOR, '#contents > div.select-paging > div.page-select.txt-num > div > select'
                    )
                    select = Select(select_element)
                    select.select_by_value(str(i + 1))
                    time.sleep(2)

                    # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                    wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '#contents > ul a')))
                    a_tags = driver.find_elements(By.CSS_SELECTOR, '#contents > ul a')
                    for a in a_tags:
                        href = a.get_attribute('href')
                        if href:
                            article_links.append(href)

                except (NoSuchElementException, StaleElementReferenceException, TimeoutException) as e:
                    print(f"í˜ì´ì§€ {i+1} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                    break

        # ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘
        categories = {
            'ê²½ì œì •ì±…': 'https://www.hankyung.com/economy/economic-policy?page=1',
            'ê±°ì‹œê²½ì œ': 'https://www.hankyung.com/economy/macro',
            'ì™¸í™˜ì‹œì¥': 'https://www.hankyung.com/economy/forex',
            'ì„¸ê¸ˆ': 'https://www.hankyung.com/economy/tax',
            'ê³ ìš©ë³µì§€': 'https://www.hankyung.com/economy/job-welfare'
        }

        for cat_name, cat_url in categories.items():
            collect_links_from_category(cat_url, cat_name)

        # ì¤‘ë³µ ì œê±°
        article_list = list(set(article_links))
        print(f"\nì´ {len(article_list)}ê°œì˜ ê¸°ì‚¬ ë§í¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

        # ë³¸ë¬¸ ìˆ˜ì§‘
        article = {}
        for i, link in enumerate(article_list):
            try:
                driver.get(link)
                print(f"[{i+1}/{len(article_list)}] ê¸°ì‚¬ í¬ë¡¤ë§ ì¤‘: {link}")
                time.sleep(2)

                article_element = wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, '#articletxt'))
                )
                text = article_element.text.strip()
                if not text:
                    print(f"ë³¸ë¬¸ ë¹„ì–´ ìˆìŒ: {link}")
                    text = "ë³¸ë¬¸ ì—†ìŒ"

                # URLì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ex: /article/2025082017461)
                date_match = re.search(r'/article/(\d{8})', link)
                if date_match:
                    published_date = f"{date_match.group(1)[:4]}-{date_match.group(1)[4:6]}-{date_match.group(1)[6:8]}"
                else:
                    published_date = None

            except (TimeoutException, NoSuchElementException, StaleElementReferenceException) as e:
                print(f"ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {link} | ì—ëŸ¬: {e}")
                text = "ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨"
                published_date = None
            except Exception as e:
                print(f"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {link} | ì—ëŸ¬: {e}")
                text = "ì ‘ê·¼ ì‹¤íŒ¨"
                published_date = None

            article[link] = {"text": text, "date": published_date, "publisher": "í•œêµ­ê²½ì œ"}

        driver.quit()
        print(f"\n[INFO] ì´ {len(article)}ê°œì˜ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ.")

        # # ë°ì´í„°í”„ë ˆì„ ë³€í™˜
        df = pd.DataFrame.from_dict(article, orient='index').reset_index()
        df.rename(columns={'index': 'url'}, inplace=True)

        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False)

        s3_hook = S3Hook(aws_conn_id='s3_conn')
        key = f"news_dataframe/korea_eco_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        s3_hook.load_string(csv_buffer.getvalue(), key=key, bucket_name=BUCKET_NAME, replace=True)
        return ''

    @task
    def save_to_s3(*news_list):
        rows = []
        for news in news_list:
            for url, data in news.items():
                rows.append({
                    "url": url,
                    "date": data.get("date"),
                    "content": data.get("content", ""),
                    "publisher": data.get("publisher", "")
                })

        df = pd.DataFrame(rows)
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False)

        s3_hook = S3Hook(aws_conn_id='s3_conn')
        s3_hook.load_string(
            string_data=csv_buffer.getvalue(),
            key="news_dataframe/news_dataframe.csv",
            bucket_name="ian-geonewsapt",
            replace=True
        )

    start = EmptyOperator(task_id='start')
    end = EmptyOperator(task_id='end')
    
    chosun_task = chosun()
    dong_a_task = dong_a()
    joonang_task = joonang()
    korea_task = korea_eco()

    #merged_task = save_to_s3(chosun_task, dong_a_task,joonang_task, korea_task)

    start >> [chosun_task, dong_a_task, joonang_task, korea_task] >> end #>> merged_task >> end